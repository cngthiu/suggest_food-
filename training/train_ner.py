"""
Script hu·∫•n luy·ªán m√¥ h√¨nh NER (Named Entity Recognition) s·ª≠ d·ª•ng SpaCy.
Input: File JSON ch·ª©a d·ªØ li·ªáu ƒë√£ g√°n nh√£n (d·∫°ng list c√°c dict).
Output: Folder model spacy (v√≠ d·ª•: serverAI/models/ner_model).
"""

import os
import json
import random
import argparse
import sys
from pathlib import Path

import spacy
from spacy.training.example import Example
from spacy.util import minibatch, compounding
from spacy.training import offsets_to_biluo_tags

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import ƒë∆∞·ª£c module serverAI t·ª´ th∆∞ m·ª•c g·ªëc
sys.path.append(os.getcwd())

try:
    from serverAI.inference.utils import norm_text
    print("‚úÖ ƒê√£ load th√†nh c√¥ng h√†m norm_text t·ª´ utils.")
except ImportError:
    print("‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y serverAI.inference.utils. Code s·∫Ω ch·∫°y m√† kh√¥ng ki·ªÉm tra chu·∫©n h√≥a.")
    def norm_text(s, **kwargs): return s


def load_data(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {path}")
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def validate_data(data):
    print(f"\n--- ƒêang ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu ({len(data)} m·∫´u) ---")
    dirty_count = 0
    for i, item in enumerate(data):
        raw_text = item.get("text", "")
        # d√πng c√πng logic v·ªõi generator
        clean_text = norm_text(raw_text, lowercase=True)

        if raw_text != clean_text:
            dirty_count += 1
            if dirty_count <= 3:
                print(f"[C·∫¢NH B√ÅO] M·∫´u #{i+1} ch∆∞a ƒë∆∞·ª£c chu·∫©n h√≥a!")
                print(f"  G·ªëc : {repr(raw_text)}")
                print(f"  Chu·∫©n: {repr(clean_text)}")
                print("  -> H√£y s·ª≠a l·∫°i file json ƒë·ªÉ text gi·ªëng d√≤ng 'Chu·∫©n', v√† c·∫≠p nh·∫≠t l·∫°i v·ªã tr√≠ entity.")

    if dirty_count > 0:
        print(f"‚ö†Ô∏è  T·ªïng c·ªông {dirty_count} m·∫´u ch∆∞a chu·∫©n h√≥a. M√¥ h√¨nh c√≥ th·ªÉ ho·∫°t ƒë·ªông k√©m ch√≠nh x√°c.")
        print("üí° G·ª£i √Ω: D√πng script chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi g√°n nh√£n.")
    else:
        print("‚úÖ D·ªØ li·ªáu s·∫°ch v√† ƒë·ªìng b·ªô v·ªõi norm_text.")


def check_alignment(nlp, data, max_show=10):
    """
    Ki·ªÉm tra entity n√†o b·ªã l·ªách token boundary.
    In ra m·ªôt s·ªë m·∫´u l·ªói ƒë·ªÉ b·∫°n s·ª≠a offset trong JSON.
    """
    print("\n--- Ki·ªÉm tra entity alignment ---")
    bad = 0

    for i, item in enumerate(data):
        text = item["text"]
        entities = item.get("entities", [])
        spans = [(start, end, label) for start, end, label in entities]
        doc = nlp.make_doc(text)

        try:
            tags = offsets_to_biluo_tags(doc, spans)
        except Exception as e:
            bad += 1
            if bad <= max_show:
                print(f"[L·ªñI HARD] M·∫´u #{i+1}: {repr(text)}")
                print("  Entities:", spans)
                print("  -> Exception:", e)
            continue

        if "-" in tags:
            bad += 1
            if bad <= max_show:
                print(f"[MISALIGNED] M·∫´u #{i+1}: {repr(text)}")
                print("  Entities:", spans)
                print("  Tags    :", tags)
                for (start, end, label) in spans:
                    print(f"    {label}: [{start}, {end}] -> {repr(text[start:end])}")

    if bad == 0:
        print("‚úÖ Kh√¥ng c√≥ entity b·ªã misaligned.")
    else:
        print(f"‚ö†Ô∏è C√≥ t·ªïng c·ªông kho·∫£ng {bad} m·∫´u c√≥ entity misaligned (hi·ªÉn th·ªã t·ªëi ƒëa {max_show}).")
        print("   -> Nh·ªØng entity n√†y s·∫Ω b·ªã spaCy b·ªè qua khi train. N√™n fix offset trong JSON.")


def evaluate_ner(nlp, data):
    """
    ƒê√°nh gi√° m√¥ h√¨nh NER tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu (dev set).
    Tr·∫£ v·ªÅ: precision, recall, f1 (cho entities).
    """
    if not data:
        return 0.0, 0.0, 0.0

    examples = []
    for item in data:
        text = item["text"]
        ents = item.get("entities", [])
        doc = nlp.make_doc(text)
        try:
            ex = Example.from_dict(doc, {"entities": ents})
            examples.append(ex)
        except Exception as e:
            # B·ªè qua m·∫´u l·ªói alignment n·∫∑ng
            continue

    if not examples:
        return 0.0, 0.0, 0.0

    scores = nlp.evaluate(examples)  # <-- tr·∫£ v·ªÅ dict
    return scores.get("ents_p", 0.0), scores.get("ents_r", 0.0), scores.get("ents_f", 0.0)



def train(data_path, output_dir, n_iter=30, drop=0.3):
    # 1. Load v√† Validate d·ªØ li·ªáu
    TRAIN_DATA = load_data(data_path)
    validate_data(TRAIN_DATA)

    # 2. Kh·ªüi t·∫°o m√¥ h√¨nh SpaCy tr·∫Øng (Blank Language Model)
    try:
        nlp = spacy.blank("vi")
        print("Load language: Vietnamese (vi)")
    except Exception:
        nlp = spacy.blank("xx")
        print("Load language: Multi-language (xx)")

    # Check alignment m·ªôt l·∫ßn tr∆∞·ªõc khi train
    check_alignment(nlp, TRAIN_DATA)

    # 3. Chia train/dev (80/20)
    random.shuffle(TRAIN_DATA)
    split = int(len(TRAIN_DATA) * 0.8)
    train_data = TRAIN_DATA[:split]
    dev_data = TRAIN_DATA[split:]
    print(f"\n--- Chia d·ªØ li·ªáu: {len(train_data)} train / {len(dev_data)} dev ---")

    # 4. T·∫°o pipeline NER
    if "ner" not in nlp.pipe_names:
        ner = nlp.add_pipe("ner", last=True)
    else:
        ner = nlp.get_pipe("ner")

    # 5. Th√™m nh√£n (Labels) v√†o m√¥ h√¨nh
    for example in TRAIN_DATA:
        for ent in example.get("entities", []):
            ner.add_label(ent[2])

    # 6. Hu·∫•n luy·ªán
    pipe_exceptions = ["ner", "trf_wordpiecer", "trf_tok2vec"]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

    print(f"\n--- B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán ({n_iter} v√≤ng) ---")
    best_f = -1.0  # ƒê·ªÉ l∆∞u best F1 tr√™n dev
    output_path = Path(output_dir)
    if not output_path.exists():
        output_path.mkdir(parents=True)

    with nlp.disable_pipes(*other_pipes):
        # V·ªõi spaCy 3.x (3.8.11), khuy·∫øn ngh·ªã d√πng initialize
        optimizer = nlp.initialize()

        for itn in range(n_iter):
            random.shuffle(train_data)
            losses = {}

            # T·∫°o l·∫°i compounding m·ªói epoch cho r√µ r√†ng
            sizes = compounding(4.0, 32.0, 1.001)
            batches = minibatch(train_data, size=sizes)

            for batch in batches:
                texts = [d["text"] for d in batch]
                annotations = [{"entities": d["entities"]} for d in batch]

                examples = []
                for text, ann in zip(texts, annotations):
                    doc = nlp.make_doc(text)
                    try:
                        example = Example.from_dict(doc, ann)
                        examples.append(example)
                    except Exception as e:
                        # B·ªè qua m·∫´u l·ªói alignment n·∫∑ng
                        print(f"[TRAIN] B·ªè qua m·∫´u l·ªói: {repr(text)} - {e}")

                if not examples:
                    continue

                nlp.update(
                    examples,
                    drop=drop,
                    losses=losses,
                    sgd=optimizer,
                )

            # ƒê√°nh gi√° dev m·ªói v√≤ng (ho·∫∑c m·ªói 5 v√≤ng n·∫øu mu·ªën gi·∫£m log)
            p, r, f = evaluate_ner(nlp, dev_data)
            msg = f"V√≤ng {itn + 1:3d} | Loss: {losses.get('ner', 0.0):8.3f} | Dev P: {p:5.3f} R: {r:5.3f} F1: {f:5.3f}"
            print(msg)

            # L∆∞u best model n·∫øu F1 t·ªët h∆°n
            if f > best_f:
                best_f = f
                best_path = output_path / "best_model"
                nlp.to_disk(best_path)
                print(f"  üëâ C·∫≠p nh·∫≠t best model (F1={best_f:.3f}) t·∫°i: {best_path}")

    # 7. L∆∞u "last model" (m√¥ h√¨nh sau epoch cu·ªëi)
    nlp.meta["name"] = "food_ner_model"
    last_model_path = output_path / "last_model"
    nlp.to_disk(last_model_path)
    print(f"\nüéâ ƒê√£ l∆∞u last model t·∫°i: {last_model_path}")
    print(f"üîé Best dev F1: {best_f:.3f} (model l∆∞u t·∫°i: {output_path / 'best_model'})")

    # 8. Test nhanh v·ªõi 1 c√¢u
    try:
        loaded_nlp = spacy.load(best_path if best_f >= 0 else last_model_path)
    except Exception:
        loaded_nlp = spacy.load(last_model_path)

    test_text = TRAIN_DATA[0]['text'] if TRAIN_DATA else "n·∫•u m√≥n c√° 3 ng∆∞·ªùi ƒÉn"
    doc = loaded_nlp(test_text)
    print(f"\nTest nhanh:")
    print(f"Input: {test_text}")
    print("Entities:", [(ent.text, ent.label_) for ent in doc.ents])


def main():
    ap = argparse.ArgumentParser(description="Train NER Model for SmartShop AI")
    ap.add_argument("--data", required=True, help="ƒê∆∞·ªùng d·∫´n file json d·ªØ li·ªáu train")
    ap.add_argument("--output", required=True, help="Th∆∞ m·ª•c l∆∞u model output")
    ap.add_argument("--iter", type=int, default=30, help="S·ªë v√≤ng l·∫∑p hu·∫•n luy·ªán (default: 30)")
    args = ap.parse_args()

    train(args.data, args.output, args.iter)


if __name__ == "__main__":
    main()

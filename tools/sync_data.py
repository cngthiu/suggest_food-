# import json
# import random
# import os
# import re
# from sklearn.model_selection import train_test_split

# # 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (S·ª≠a theo m√¥i tr∆∞·ªùng c·ªßa b·∫°n)
# DATA_DIR = "serverAI/data/nlu"
# GAZETTEER_DIR = os.path.join(DATA_DIR, "gazetteer")
# NUM_SAMPLES = 2500  # T·ªïng s·ªë m·∫´u sinh ra
# SEED = 42

# # 2. H√ÄM LOAD GAZETTEER T·ª™ FILE
# def load_gazetteer(file_name):
#     path = os.path.join(GAZETTEER_DIR, file_name)
#     if not os.path.exists(path):
#         return []
#     with open(path, 'r', encoding='utf-8') as f:
#         content = f.read()
#         # Lo·∫°i b·ªè c√°c tag v√† t√°ch t·ª´ d·ª±a tr√™n d·∫•u | ho·∫∑c xu·ªëng d√≤ng
#         lines = re.sub(r'\\', '', content).split('\n')
#         words = []
#         for line in lines:
#             parts = line.split('|')
#             words.extend([p.strip().lower() for p in parts if p.strip()])
#         return list(set(words))

# # 3. KHO D·ªÆ LI·ªÜU PHONG PH√ö
# DATA_POOL = {
#     "food": load_gazetteer("protein.txt"),
#     "diet": load_gazetteer("diet.txt"),
#     "device": load_gazetteer("device.txt"),
#     "time": ["15 ph√∫t", "30p", "1 ti·∫øng", "45 ph√∫t", "nhanh", "c·∫•p t·ªëc", "20 ph√∫t", "1h", "t·∫ßm 30 ph√∫t"],
#     "price": ["50k", "100.000ƒë", "gi√° r·∫ª", "200 ngh√¨n", "v·ª´a t√∫i ti·ªÅn", "t·∫ßm 70k", "150 ng√†n"],
#     "quantity": ["2 ng∆∞·ªùi", "4 ph·∫ßn ƒÉn", "c·∫£ nh√†", "3 th√†nh vi√™n", "1 su·∫•t", "gia ƒë√¨nh 4 ng∆∞·ªùi", "cho 2 b√©"]
# }

# # 4. TEMPLATES CHO 6 INTENTS
# TEMPLATES = [
#     # search_recipe
#     {"intent": "search_recipe", "tmpl": "t√¨m c√°ch n·∫•u {food} {diet}"},
#     {"intent": "search_recipe", "tmpl": "g·ª£i √Ω m√≥n {food} l√†m trong {time}"},
#     {"intent": "search_recipe", "tmpl": "n·∫•u m√≥n g√¨ t·ª´ {food} cho {quantity}"},
#     {"intent": "search_recipe", "tmpl": "mu·ªën ƒÉn {food} {diet} kho·∫£ng {price}"},
    
#     # ask_recipe_detail
#     {"intent": "ask_recipe_detail", "tmpl": "h∆∞·ªõng d·∫´n l√†m m√≥n {food}"},
#     {"intent": "ask_recipe_detail", "tmpl": "c√¥ng th·ª©c n·∫•u {food} chi ti·∫øt"},
#     {"intent": "ask_recipe_detail", "tmpl": "c√°ch ch·∫ø bi·∫øn {food} nh∆∞ th·∫ø n√†o"},
    
#     # refine_search
#     {"intent": "refine_search", "tmpl": "nh∆∞ng m√¨nh mu·ªën d√πng {device}"},
#     {"intent": "refine_search", "tmpl": "t√¨m l·∫°i m√≥n {food} cho {quantity}"},
#     {"intent": "refine_search", "tmpl": "th√™m ƒëi·ªÅu ki·ªán l√† {diet}"},
    
#     # add_ingredients_to_cart
#     {"intent": "add_ingredients_to_cart", "tmpl": "mua nguy√™n li·ªáu n·∫•u {food}"},
#     {"intent": "add_ingredients_to_cart", "tmpl": "cho th·ª±c ph·∫©m l√†m {food} v√†o gi·ªè"},
#     {"intent": "add_ingredients_to_cart", "tmpl": "ƒë·∫∑t h√†ng nguy√™n li·ªáu cho m√≥n {food}"},
    
#     # ask_price_estimate
#     {"intent": "ask_price_estimate", "tmpl": "n·∫•u {food} cho {quantity} h·∫øt bao nhi√™u"},
#     {"intent": "ask_price_estimate", "tmpl": "chi ph√≠ l√†m {food} kho·∫£ng {price} ƒë√∫ng kh√¥ng"},
#     {"intent": "ask_price_estimate", "tmpl": "gi√° nguy√™n li·ªáu m√≥n {food} hi·ªán nay"},
    
#     # fallback
#     {"intent": "fallback", "tmpl": "xin ch√†o"},
#     {"intent": "fallback", "tmpl": "b·∫°n c√≥ th·ªÉ l√†m g√¨"},
#     {"intent": "fallback", "tmpl": "th·ªùi ti·∫øt h√¥m nay th·∫ø n√†o"},
# ]

# LABEL_MAP = {
#     "food": "FOOD", "diet": "DIET", "time": "TIME", 
#     "price": "PRICE", "quantity": "QUANTITY", "device": "DEVICE"
# }

# def generate_samples(num_samples):
#     samples = []
#     seen_texts = set()
    
#     while len(samples) < num_samples:
#         t_obj = random.choice(TEMPLATES)
#         tmpl = t_obj["tmpl"]
#         intent = t_obj["intent"]
        
#         placeholders = re.findall(r"\{(.*?)\}", tmpl)
#         text = tmpl
#         entities = []
        
#         # S·∫Øp x·∫øp placeholders ƒë·ªÉ thay th·∫ø kh√¥ng l√†m l·ªách index c·ªßa c√°c placeholder sau
#         # Tuy nhi√™n ·ªü ƒë√¢y d√πng replace 1 l·∫ßn duy nh·∫•t cho m·ªói placeholder l√† an to√†n
#         for p in placeholders:
#             val = random.choice(DATA_POOL[p])
#             start_idx = text.find("{" + p + "}")
#             text = text.replace("{" + p + "}", val, 1)
#             end_idx = start_idx + len(val)
#             entities.append([start_idx, end_idx, LABEL_MAP[p]])
            
#         if text not in seen_texts:
#             samples.append({
#                 "text": text,
#                 "intent": intent, # L∆∞u l·∫°i intent ƒë·ªÉ split stratified
#                 "entities": entities
#             })
#             seen_texts.add(text)
#     return samples

# def main():
#     print("üöÄ B·∫Øt ƒë·∫ßu sinh d·ªØ li·ªáu NER...")
#     all_data = generate_samples(NUM_SAMPLES)
    
#     # Chia t·∫≠p Train/Valid 80/20 c√≥ ph√¢n l·ªõp (Stratified) theo Intent
#     intents_labels = [s["intent"] for s in all_data]
#     train_data, valid_data = train_test_split(
#         all_data, 
#         test_size=0.2, 
#         random_state=SEED, 
#         stratify=intents_labels
#     )
    
#     # Lo·∫°i b·ªè tr∆∞·ªùng 'intent' trong file JSON cu·ªëi c√πng (v√¨ NER ch·ªâ c·∫ßn text v√† entities)
#     for s in train_data: s.pop("intent")
#     for s in valid_data: s.pop("intent")

#     # L∆∞u file
#     os.makedirs(DATA_DIR, exist_ok=True)
#     with open(os.path.join(DATA_DIR, 'ner_train.json'), 'w', encoding='utf-8') as f:
#         json.dump(train_data, f, ensure_ascii=False, indent=2)
    
#     with open(os.path.join(DATA_DIR, 'ner_valid.json'), 'w', encoding='utf-8') as f:
#         json.dump(valid_data, f, ensure_ascii=False, indent=2)

#     print(f"‚úÖ Ho√†n th√†nh!")
#     print(f" - T·ªïng: {len(all_data)} m·∫´u")
#     print(f" - Train: {len(train_data)} m·∫´u t·∫°i {DATA_DIR}/ner_train.json")
#     print(f" - Valid: {len(valid_data)} m·∫´u t·∫°i {DATA_DIR}/ner_valid.json")

# if __name__ == "__main__":
#     main()
import json
import random
import os
import re
from sklearn.model_selection import train_test_split
from unidecode import unidecode

# 1. C·∫§U H√åNH
DATA_DIR = "serverAI/data/nlu"
GAZETTEER_DIR = os.path.join(DATA_DIR, "gazetteer")
RECIPE_FILE = "/media/congthieu/ubuntu_data/LTTM/MM/serverAI/data/recipes/recipies.json"
TOTAL_SAMPLES = 2000 
SEED = 42

# 2. LOAD DATA (Gi·ªØ nguy√™n logic c≈©)
def load_gazetteer(file_name):
    path = os.path.join(GAZETTEER_DIR, file_name)
    if not os.path.exists(path): return []
    with open(path, 'r', encoding='utf-8') as f:
        content = f.read()
        lines = re.sub(r'\\', '', content).split('\n')
        words = []
        for line in lines:
            parts = line.split('|')
            words.extend([p.strip().lower() for p in parts if p.strip()])
        return list(set(words))

def extract_from_recipes(file_path):
    if not os.path.exists(file_path): return [], []
    with open(file_path, 'r', encoding='utf-8') as f:
        recipes = json.load(f)
    food_items = []
    diets = []
    for r in recipes:
        food_items.append(r['title'].lower())
        if 'search_keywords' in r: food_items.extend([kw.lower() for kw in r['search_keywords']])
        if 'diet' in r: diets.extend([d.lower() for d in r['diet']])
        for ing in r.get('ingredients', []): food_items.append(ing['name'].lower())
    return list(set(food_items)), list(set(diets))

# 3. KHO D·ªÆ LI·ªÜU T·ªîNG H·ª¢P
raw_food, raw_diet = extract_from_recipes(RECIPE_FILE)
DATA_POOL = {
    "food": list(set(load_gazetteer("protein.txt") + raw_food)),
    "diet": list(set(load_gazetteer("diet.txt") + raw_diet)),
    "device": load_gazetteer("device.txt"),
    "time": ["10 ph√∫t", "15p", "30 ph√∫t", "1 ti·∫øng", "45 ph√∫t", "20 ph√∫t", "nhanh", "c·∫•p t·ªëc", "si√™u t·ªëc"],
    "price": ["50k", "100.000ƒë", "gi√° r·∫ª", "200 ngh√¨n", "v·ª´a t√∫i ti·ªÅn", "t·∫ßm 80k", "sinh vi√™n", "gi√° b√¨nh d√¢n"],
    "quantity": ["2 ng∆∞·ªùi", "4 ph·∫ßn ƒÉn", "c·∫£ nh√†", "3 th√†nh vi√™n", "1 su·∫•t", "gia ƒë√¨nh 4 ng∆∞·ªùi", "cho b√©", "cho 2 ng∆∞·ªùi"]
}

LABEL_MAP = {
    "food": "FOOD", "diet": "DIET", "time": "TIME", 
    "price": "PRICE", "quantity": "QUANTITY", "device": "DEVICE"
}

# 4. TEMPLATES PH√ÇN LO·∫†I THEO NH√ÉN
# Ch√∫ng ta li·ªát k√™ c√°c template v√† ƒë√°nh d·∫•u c√°c nh√£n m√† n√≥ ch·ª©a
TEMPLATES = [
    {"intent": "search_recipe", "tmpl": "t√¨m c√°ch n·∫•u {food} {diet}", "labels": ["food", "diet"]},
    {"intent": "search_recipe", "tmpl": "g·ª£i √Ω m√≥n {food} l√†m trong {time}", "labels": ["food", "time"]},
    {"intent": "search_recipe", "tmpl": "mu·ªën ƒÉn {food} {diet} t·∫ßm {price}", "labels": ["food", "diet", "price"]},
    {"intent": "ask_recipe_detail", "tmpl": "h∆∞·ªõng d·∫´n l√†m m√≥n {food}", "labels": ["food"]},
    {"intent": "refine_search", "tmpl": "nh∆∞ng m√¨nh mu·ªën d√πng {device}", "labels": ["device"]},
    {"intent": "refine_search", "tmpl": "t√¨m l·∫°i m√≥n {food} cho {quantity}", "labels": ["food", "quantity"]},
    {"intent": "add_ingredients_to_cart", "tmpl": "mua nguy√™n li·ªáu n·∫•u {food}", "labels": ["food"]},
    {"intent": "ask_price_estimate", "tmpl": "n·∫•u {food} cho {quantity} h·∫øt bao nhi√™u", "labels": ["food", "quantity"]},
    {"intent": "ask_price_estimate", "tmpl": "chi ph√≠ l√†m {food} kho·∫£ng {price}", "labels": ["food", "price"]},
    {"intent": "search_recipe", "tmpl": "c√≥ m√≥n {food} n√†o l√†m b·∫±ng {device} m·∫•t {time} kh√¥ng", "labels": ["food", "device", "time"]},
    {"intent": "search_recipe", "tmpl": "th·ª±c ƒë∆°n {diet} cho {quantity} gi√° {price}", "labels": ["diet", "quantity", "price"]}
]

# 5. LOGIC SINH D·ªÆ LI·ªÜU C√ÇN B·∫∞NG
def generate_balanced_samples(total_samples):
    samples = []
    seen_texts = set()
    # Kh·ªüi t·∫°o b·ªô ƒë·∫øm nh√£n
    label_counts = {l: 0 for l in LABEL_MAP.values()}
    
    print("üîÑ ƒêang sinh d·ªØ li·ªáu c√¢n b·∫±ng...")
    
    while len(samples) < total_samples:
        # T√¨m nh√£n ƒëang c√≥ √≠t m·∫´u nh·∫•t
        min_label_key = min(label_counts, key=label_counts.get)
        
        # L·ªçc c√°c template c√≥ ch·ª©a nh√£n ƒëang thi·∫øu n√†y
        suitable_templates = [t for t in TEMPLATES if any(LABEL_MAP[l] == min_label_key for l in t["labels"])]
        
        # N·∫øu kh√¥ng c√≥ template n√†o ch·ª©a nh√£n ƒë√≥ (l·ªói logic), ch·ªçn ng·∫´u nhi√™n
        if not suitable_templates:
            t_obj = random.choice(TEMPLATES)
        else:
            t_obj = random.choice(suitable_templates)
            
        text = t_obj["tmpl"]
        entities = []
        placeholders = re.findall(r"\{(.*?)\}", text)
        
        for p in placeholders:
            val = random.choice(DATA_POOL[p])
            start = text.find("{" + p + "}")
            text = text.replace("{" + p + "}", val, 1)
            label_name = LABEL_MAP[p]
            entities.append([start, start + len(val), label_name])
            label_counts[label_name] += 1 # C·∫≠p nh·∫≠t b·ªô ƒë·∫øm khi sinh ra nh√£n
            
        # Augmentation (25% kh√¥ng d·∫•u)
        if random.random() < 0.25:
            text = unidecode(text)
            
        if text not in seen_texts:
            samples.append({"text": text, "intent": t_obj["intent"], "entities": entities})
            seen_texts.add(text)
            
    print("üìä Th·ªëng k√™ nh√£n sau khi sinh:")
    for l, c in label_counts.items():
        print(f" - {l}: {c}")
    return samples

# 6. TH·ª∞C THI
def main():
    random.seed(SEED)
    all_data = generate_balanced_samples(TOTAL_SAMPLES)
    
    train_data, valid_data = train_test_split(
        all_data, test_size=0.15, random_state=SEED, 
        stratify=[s["intent"] for s in all_data]
    )
    
    for s in train_data + valid_data: s.pop("intent")

    os.makedirs(DATA_DIR, exist_ok=True)
    with open(os.path.join(DATA_DIR, 'ner_train.json'), 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    with open(os.path.join(DATA_DIR, 'ner_valid.json'), 'w', encoding='utf-8') as f:
        json.dump(valid_data, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu c√¢n b·∫±ng v√†o ner_train.json v√† ner_valid.json")

if __name__ == "__main__":
    main()